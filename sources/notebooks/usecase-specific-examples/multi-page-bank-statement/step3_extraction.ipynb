{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Information Extraction\n",
    "\n",
    "This notebook performs information extraction from classified document sections using AWS Bedrock.\n",
    "\n",
    "**Inputs:**\n",
    "- Document object with classification results from Step 2\n",
    "- Extraction configuration\n",
    "- Document classes with attributes definition\n",
    "\n",
    "**Outputs:**\n",
    "- Document with extraction results for each section\n",
    "- Structured data extracted based on document class attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Step Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.models import Document, Status\n",
    "from idp_common import extraction\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('idp_common.extraction').setLevel(logging.INFO)\n",
    "logging.getLogger('idp_common.bedrock.client').setLevel(logging.INFO)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document from previous step\n",
    "classification_data_dir = Path(\".data/step2_classification\")\n",
    "\n",
    "# Load document object from JSON\n",
    "document_path = classification_data_dir / \"document.json\"\n",
    "with open(document_path, 'r') as f:\n",
    "    document = Document.from_json(f.read())\n",
    "\n",
    "# Load configuration directly from config files\n",
    "import yaml\n",
    "config_dir = Path(\"config\")\n",
    "CONFIG = {}\n",
    "\n",
    "# Load each configuration file\n",
    "config_files = [\n",
    "    \"extraction.yaml\",\n",
    "    \"classes.yaml\"\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    config_path = config_dir / config_file\n",
    "    if config_path.exists():\n",
    "        with open(config_path, 'r') as f:\n",
    "            file_config = yaml.safe_load(f)\n",
    "            CONFIG.update(file_config)\n",
    "        print(f\"Loaded {config_file}\")\n",
    "    else:\n",
    "        print(f\"Warning: {config_file} not found\")\n",
    "\n",
    "# Load environment info\n",
    "env_path = classification_data_dir / \"environment.json\"\n",
    "with open(env_path, 'r') as f:\n",
    "    env_info = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['AWS_REGION'] = env_info['region']\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Modular-Pipeline'\n",
    "\n",
    "print(f\"Loaded document: {document.id}\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of sections: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"Loaded configuration sections: {list(CONFIG.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Extraction Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract extraction configuration\n",
    "extraction_config = CONFIG.get('extraction', {})\n",
    "print(\"Extraction Configuration:\")\n",
    "print(f\"Model: {extraction_config.get('model')}\")\n",
    "print(f\"Temperature: {extraction_config.get('temperature')}\")\n",
    "print(f\"Max Tokens: {extraction_config.get('max_tokens')}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "print(f\"System Prompt:\\n{extraction_config.get('system_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"Task Prompt:\\n{extraction_config.get('task_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display available document classes and their attributes\n",
    "classes = CONFIG.get('classes', [])\n",
    "print(f\"\\nDocument Classes and Attributes:\")\n",
    "for cls in classes:\n",
    "    print(f\"\\n{cls['name']} ({len(cls.get('attributes', []))} attributes):\")\n",
    "    for attr in cls.get('attributes', [])[:3]:  # Show first 3 attributes\n",
    "        print(f\"  - {attr['name']}: {attr['description'][:100]}...\")\n",
    "    if len(cls.get('attributes', [])) > 3:\n",
    "        print(f\"  ... and {len(cls.get('attributes', [])) - 3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extraction service with Bedrock\n",
    "extraction_service = extraction.ExtractionService(config=CONFIG)\n",
    "\n",
    "print(\"Extraction service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Information from Document Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse S3 URIs and load JSON\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def load_json_from_s3(uri):\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting information from document sections...\")\n",
    "\n",
    "if not document.sections:\n",
    "    print(\"No sections found in document. Cannot proceed with extraction.\")\n",
    "else:\n",
    "    extraction_results = []\n",
    "    \n",
    "    # Process each section (limit to first 3 to save time in demo)\n",
    "    n = min(3, len(document.sections))\n",
    "    print(f\"Processing first {n} of {len(document.sections)} sections...\")\n",
    "    \n",
    "    for i, section in enumerate(document.sections[:n]):\n",
    "        print(f\"\\n--- Processing Section {i+1}/{n} ---\")\n",
    "        print(f\"Section ID: {section.section_id}\")\n",
    "        print(f\"Classification: {section.classification}\")\n",
    "        print(f\"Pages: {section.page_ids}\")\n",
    "        \n",
    "        # Process section extraction\n",
    "        start_time = time.time()\n",
    "        document = extraction_service.process_document_section(\n",
    "            document=document,\n",
    "            section_id=section.section_id\n",
    "        )\n",
    "        extraction_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Extraction completed in {extraction_time:.2f} seconds\")\n",
    "        \n",
    "        # Record results\n",
    "        extraction_results.append({\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'processing_time': extraction_time,\n",
    "            'extraction_result_uri': getattr(section, 'extraction_result_uri', None)\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nExtraction complete for {n} sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Extraction Results ===\")\n",
    "\n",
    "if document.sections:\n",
    "    for i, section in enumerate(document.sections[:n]):\n",
    "        print(f\"\\n--- Section {section.section_id} ({section.classification}) ---\")\n",
    "        \n",
    "        if hasattr(section, 'extraction_result_uri') and section.extraction_result_uri:\n",
    "            try:\n",
    "                # Load extraction results from S3\n",
    "                extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "                \n",
    "                print(f\"Extraction Result URI: {section.extraction_result_uri}\")\n",
    "                \n",
    "                # Display inference results\n",
    "                if 'inference_result' in extraction_data:\n",
    "                    inference_result = extraction_data['inference_result']\n",
    "                    print(\"Extracted Data:\")\n",
    "                    for attr_name, attr_value in inference_result.items():\n",
    "                        if attr_value is not None:\n",
    "                            # Truncate long values for display\n",
    "                            display_value = str(attr_value)[:1000] + \"...\" if len(str(attr_value)) > 1000 else attr_value\n",
    "                            print(f\"  {attr_name}: {display_value}\")\n",
    "                        else:\n",
    "                            print(f\"  {attr_name}: null\")\n",
    "                else:\n",
    "                    print(\"No inference results found\")\n",
    "                    \n",
    "                # Display metadata if available\n",
    "                if 'metadata' in extraction_data:\n",
    "                    metadata = extraction_data['metadata']\n",
    "                    print(f\"Processing time: {metadata.get('extraction_time_seconds', 'N/A')} seconds\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading extraction results: {e}\")\n",
    "        else:\n",
    "            print(\"No extraction results available\")\n",
    "else:\n",
    "    print(\"No sections to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results for Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory for this step\n",
    "data_dir = Path(\".data/step3_extraction\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save updated document object as JSON\n",
    "document_path = data_dir / \"document.json\"\n",
    "with open(document_path, 'w') as f:\n",
    "    f.write(document.to_json())\n",
    "\n",
    "# Save configuration (pass through)\n",
    "config_path = data_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save environment info (pass through)\n",
    "env_path = data_dir / \"environment.json\"\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "# Save extraction-specific results summary\n",
    "extraction_summary = {\n",
    "    'model_used': extraction_config.get('model'),\n",
    "    'sections_processed': len(extraction_results) if 'extraction_results' in locals() else 0,\n",
    "    'total_sections': len(document.sections) if document.sections else 0,\n",
    "    'section_results': extraction_results if 'extraction_results' in locals() else [],\n",
    "    'sections_with_extractions': [\n",
    "        {\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'extraction_result_uri': getattr(section, 'extraction_result_uri', None),\n",
    "            'has_results': hasattr(section, 'extraction_result_uri') and section.extraction_result_uri is not None\n",
    "        } for section in (document.sections or [])\n",
    "    ]\n",
    "}\n",
    "\n",
    "extraction_summary_path = data_dir / \"extraction_summary.json\"\n",
    "with open(extraction_summary_path, 'w') as f:\n",
    "    json.dump(extraction_summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved document to: {document_path}\")\n",
    "print(f\"Saved configuration to: {config_path}\")\n",
    "print(f\"Saved environment info to: {env_path}\")\n",
    "print(f\"Saved extraction summary to: {extraction_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_processed = len(extraction_results) if 'extraction_results' in locals() else 0\n",
    "sections_with_results = sum(1 for section in (document.sections or []) if hasattr(section, 'extraction_result_uri') and section.extraction_result_uri)\n",
    "\n",
    "print(\"=== Step 3: Extraction Complete ===\")\n",
    "print(f\"✅ Document processed: {document.id}\")\n",
    "print(f\"✅ Sections processed: {sections_processed} of {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"✅ Sections with results: {sections_with_results}\")\n",
    "print(f\"✅ Model used: {extraction_config.get('model')}\")\n",
    "print(f\"✅ Data saved to: .data/step3_extraction/\")\n",
    "print(\"\\n📌 Next step: Run step4_assessment.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
