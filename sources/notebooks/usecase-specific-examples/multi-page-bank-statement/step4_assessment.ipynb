{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Assessment\n",
    "\n",
    "This notebook performs assessment of extraction results, evaluating confidence and accuracy using AWS Bedrock.\n",
    "\n",
    "**Inputs:**\n",
    "- Document object with extraction results from Step 3\n",
    "- Assessment configuration\n",
    "- Document classes with confidence thresholds\n",
    "\n",
    "**Outputs:**\n",
    "- Document with assessment results for each extraction\n",
    "- Confidence scores and reasoning for extracted attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Step Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.models import Document, Status\n",
    "from idp_common import assessment\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('idp_common.assessment.service').setLevel(logging.DEBUG)\n",
    "logging.getLogger('idp_common.bedrock.client').setLevel(logging.DEBUG)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document from previous step\n",
    "extraction_data_dir = Path(\".data/step3_extraction\")\n",
    "\n",
    "# Load document object from JSON\n",
    "document_path = extraction_data_dir / \"document.json\"\n",
    "with open(document_path, 'r') as f:\n",
    "    document = Document.from_json(f.read())\n",
    "\n",
    "# Load configuration directly from config files\n",
    "import yaml\n",
    "config_dir = Path(\"config\")\n",
    "CONFIG = {}\n",
    "\n",
    "# Load each configuration file\n",
    "config_files = [\n",
    "    \"assessment.yaml\",\n",
    "    \"classes.yaml\"\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    config_path = config_dir / config_file\n",
    "    if config_path.exists():\n",
    "        with open(config_path, 'r') as f:\n",
    "            file_config = yaml.safe_load(f)\n",
    "            CONFIG.update(file_config)\n",
    "        print(f\"Loaded {config_file}\")\n",
    "    else:\n",
    "        print(f\"Warning: {config_file} not found\")\n",
    "\n",
    "# Load environment info\n",
    "env_path = extraction_data_dir / \"environment.json\"\n",
    "with open(env_path, 'r') as f:\n",
    "    env_info = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['AWS_REGION'] = env_info['region']\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Modular-Pipeline'\n",
    "\n",
    "print(f\"Loaded document: {document.id}\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of sections: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"Loaded configuration sections: {list(CONFIG.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Assessment Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract assessment configuration\n",
    "assessment_config = CONFIG.get('assessment', {})\n",
    "print(\"Assessment Configuration:\")\n",
    "print(f\"Model: {assessment_config.get('model')}\")\n",
    "print(f\"Temperature: {assessment_config.get('temperature')}\")\n",
    "print(f\"Max Tokens: {assessment_config.get('max_tokens')}\")\n",
    "print(f\"Default Confidence Threshold: {assessment_config.get('default_confidence_threshold')}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"System Prompt:\\n{assessment_config.get('system_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"Task Prompt:\\n{assessment_config.get('task_prompt')}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Display document classes with confidence thresholds\n",
    "classes = CONFIG.get('classes', [])\n",
    "print(f\"\\nDocument Classes with Confidence Thresholds:\")\n",
    "for cls in classes:\n",
    "    print(f\"\\n{cls['name']}:\")\n",
    "    for attr in cls.get('attributes', [])[:5]:  # Show first 5 attributes\n",
    "        threshold = attr.get('confidence_threshold', 'default')\n",
    "        print(f\"  - {attr['name']}: threshold = {threshold}\")\n",
    "    if len(cls.get('attributes', [])) > 5:\n",
    "        print(f\"  ... and {len(cls.get('attributes', [])) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create assessment service with Bedrock\n",
    "assessment_service = assessment.AssessmentService(config=CONFIG)\n",
    "\n",
    "print(\"Assessment service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assess Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse S3 URIs and load JSON\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def load_json_from_s3(uri):\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assessing extraction confidence for document sections...\")\n",
    "\n",
    "if not document.sections:\n",
    "    print(\"No sections found in document. Cannot proceed with assessment.\")\n",
    "else:\n",
    "    assessment_results = []\n",
    "    \n",
    "    # Process each section that has extraction results (limit to first 3 to save time)\n",
    "    sections_with_extractions = [s for s in document.sections if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri]\n",
    "    n = min(3, len(sections_with_extractions))\n",
    "    \n",
    "    print(f\"Found {len(sections_with_extractions)} sections with extraction results\")\n",
    "    print(f\"Processing first {n} sections for assessment...\")\n",
    "    \n",
    "    for i, section in enumerate(sections_with_extractions[:n]):\n",
    "        print(f\"\\n--- Assessing Section {i+1}/{n} ---\")\n",
    "        print(f\"Section ID: {section.section_id}\")\n",
    "        print(f\"Classification: {section.classification}\")\n",
    "        print(f\"Extraction Result URI: {section.extraction_result_uri}\")\n",
    "        \n",
    "        # Process section assessment\n",
    "        start_time = time.time()\n",
    "        document = assessment_service.process_document_section(\n",
    "            document=document,\n",
    "            section_id=section.section_id\n",
    "        )\n",
    "        assessment_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Assessment completed in {assessment_time:.2f} seconds\")\n",
    "        \n",
    "        # Record results\n",
    "        assessment_results.append({\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'processing_time': assessment_time,\n",
    "            'extraction_result_uri': section.extraction_result_uri\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nAssessment complete for {n} sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Assessment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_assessment_data(data, attr_name=\"\", indent=\"  \"):\n",
    "    \"\"\"\n",
    "    Recursively display assessment data supporting simple, group, and list attributes.\n",
    "    \n",
    "    Args:\n",
    "        data: Assessment data (can be dict with confidence, dict with nested attrs, or list)\n",
    "        attr_name: Name of the current attribute for display\n",
    "        indent: Current indentation level\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Check if this is a confidence assessment (has 'confidence' key)\n",
    "        if 'confidence' in data:          \n",
    "            print(f\"{indent}{attr_name}: {json.dumps(data)}\")\n",
    "        else:\n",
    "            # This is a group attribute - iterate through sub-attributes\n",
    "            print(f\"{indent}{attr_name} (Group):\")\n",
    "            for sub_attr_name, sub_data in data.items():\n",
    "                display_assessment_data(sub_data, sub_attr_name, indent + \"  \")\n",
    "                \n",
    "    elif isinstance(data, list):\n",
    "        # This is a list attribute - display each item\n",
    "        print(f\"{indent}{attr_name} (List - {len(data)} items):\")\n",
    "        for i, item_data in enumerate(data):\n",
    "            print(f\"{indent}  Item {i+1}:\")\n",
    "            if isinstance(item_data, dict):\n",
    "                for item_attr_name, item_assessment in item_data.items():\n",
    "                    display_assessment_data(item_assessment, item_attr_name, indent + \"    \")\n",
    "            else:\n",
    "                print(f\"{indent}    Unexpected item format: {type(item_data)}\")\n",
    "    else:\n",
    "        print(f\"{indent}{attr_name}: Unexpected data type {type(data)}\")\n",
    "\n",
    "print(\"Assessment display helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Assessment Results ===\")\n",
    "\n",
    "if document.sections:\n",
    "    sections_with_extractions = [s for s in document.sections if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri]\n",
    "    n = min(3, len(sections_with_extractions))\n",
    "    \n",
    "    for i, section in enumerate(sections_with_extractions[:n]):\n",
    "        print(f\"\\n--- Section {section.section_id} ({section.classification}) ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load the updated extraction results with assessment\n",
    "            extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "            \n",
    "            print(f\"Extraction Result URI: {section.extraction_result_uri}\")\n",
    "                       \n",
    "            # Display the assessment results with support for nested structures\n",
    "            explainability_info = extraction_data.get('explainability_info', [])\n",
    "            if explainability_info:\n",
    "                print(\"\\nAssessment Results:\")\n",
    "                # The explainability_info is a list, get the first item which contains the assessments\n",
    "                assessments = explainability_info[0] if isinstance(explainability_info, list) else explainability_info\n",
    "                \n",
    "                for attr_name, assessment_data in assessments.items():\n",
    "                    display_assessment_data(assessment_data, attr_name)\n",
    "            else:\n",
    "                print(\"\\nNo assessment results found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading assessment results: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print(\"No sections to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display raw explainability_info for debugging if needed\n",
    "if 'explainability_info' in locals():\n",
    "    print(\"\\n=== Raw Assessment Data (for debugging) ===\")\n",
    "    print(json.dumps(explainability_info, indent=2)[:1000] + \"...\" if len(json.dumps(explainability_info)) > 1000 else json.dumps(explainability_info, indent=2))\n",
    "else:\n",
    "    print(\"No explainability_info available for debugging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results for Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory for this step\n",
    "data_dir = Path(\".data/step4_assessment\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save updated document object as JSON\n",
    "document_path = data_dir / \"document.json\"\n",
    "with open(document_path, 'w') as f:\n",
    "    f.write(document.to_json())\n",
    "\n",
    "# Save configuration (pass through)\n",
    "config_path = data_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save environment info (pass through)\n",
    "env_path = data_dir / \"environment.json\"\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "# Save assessment-specific results summary\n",
    "assessment_summary = {\n",
    "    'model_used': assessment_config.get('model'),\n",
    "    'default_confidence_threshold': assessment_config.get('default_confidence_threshold'),\n",
    "    'sections_assessed': len(assessment_results) if 'assessment_results' in locals() else 0,\n",
    "    'total_sections_with_extractions': len([s for s in (document.sections or []) if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri]),\n",
    "    'assessment_results': assessment_results if 'assessment_results' in locals() else [],\n",
    "    'sections_status': [\n",
    "        {\n",
    "            'section_id': section.section_id,\n",
    "            'classification': section.classification,\n",
    "            'has_extraction': hasattr(section, 'extraction_result_uri') and section.extraction_result_uri is not None,\n",
    "            'extraction_result_uri': getattr(section, 'extraction_result_uri', None)\n",
    "        } for section in (document.sections or [])\n",
    "    ]\n",
    "}\n",
    "\n",
    "assessment_summary_path = data_dir / \"assessment_summary.json\"\n",
    "with open(assessment_summary_path, 'w') as f:\n",
    "    json.dump(assessment_summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved document to: {document_path}\")\n",
    "print(f\"Saved configuration to: {config_path}\")\n",
    "print(f\"Saved environment info to: {env_path}\")\n",
    "print(f\"Saved assessment summary to: {assessment_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_assessed = len(assessment_results) if 'assessment_results' in locals() else 0\n",
    "sections_with_extractions = len([s for s in (document.sections or []) if hasattr(s, 'extraction_result_uri') and s.extraction_result_uri])\n",
    "\n",
    "print(\"=== Step 4: Assessment Complete ===\")\n",
    "print(f\"✅ Document processed: {document.id}\")\n",
    "print(f\"✅ Sections assessed: {sections_assessed} of {sections_with_extractions} with extractions\")\n",
    "print(f\"✅ Total sections: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"✅ Model used: {assessment_config.get('model')}\")\n",
    "print(f\"✅ Default threshold: {assessment_config.get('default_confidence_threshold')}\")\n",
    "print(f\"✅ Data saved to: .data/step4_assessment/\")\n",
    "meteringkey = f\"Assessment/bedrock/{assessment_config.get('model')}\"\n",
    "print(f\"✅ Token usage: {document.metering[meteringkey]}\")\n",
    "\n",
    "print(\"\\n📌 Next step: Run step5_summarization.ipynb\")\n",
    "print(\"\\n📋 Assessment Features Demonstrated:\")\n",
    "print(\"  • Simple attribute confidence assessment\")\n",
    "print(\"  • Group attribute nested confidence display\")\n",
    "print(\"  • List attribute individual item assessments\")\n",
    "print(\"  • Confidence threshold tracking and alerts\")\n",
    "print(\"  • Structured assessment result display\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
