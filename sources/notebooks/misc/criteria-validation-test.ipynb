{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria Validation Testing Notebook\n",
    "\n",
    "This notebook demonstrates how to use the criteria validation module for healthcare/insurance prior authorization validation.\n",
    "\n",
    "The criteria validation module:\n",
    "1. Processes user history documents from S3\n",
    "2. Validates them against configurable criteria questions\n",
    "3. Generates recommendations (Pass/Fail/Information Not Found)\n",
    "4. Supports async processing with rate limiting\n",
    "5. Tracks costs and metering data\n",
    "6. **Now uses centralized config.yaml for all configuration**\n",
    "\n",
    "> **Note**: This notebook uses AWS services including S3 and Bedrock. You need valid AWS credentials with appropriate permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ROOTDIR=\"../\"\n",
    "\n",
    "# First uninstall existing package\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package including criteria validation\n",
    "%pip install -q -e \"{ROOTDIR}/lib/idp_common_pkg[all]\"\n",
    "\n",
    "# Install additional dependencies including nest_asyncio for Jupyter async support\n",
    "%pip install -q pydantic nest_asyncio pyyaml\n",
    "\n",
    "# Check installed version\n",
    "%pip show idp_common | grep -E \"Version|Location\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import datetime\n",
    "import asyncio\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Fix for Jupyter async event loop conflicts\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import criteria validation module\n",
    "from idp_common.criteria_validation import CriteriaValidationService, CriteriaValidationResult\n",
    "\n",
    "# Load configuration from YAML file\n",
    "config_path = \"../../config_library/pattern-2/criteria-validation/config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "print(f\"✅ Loaded configuration from: {config_path}\")\n",
    "print(f\"Available criteria types: {config_data.get('criteria_types', [])}\")\n",
    "print(f\"Model: {config_data['criteria_validation']['model']}\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('idp_common.criteria_validation').setLevel(logging.DEBUG)\n",
    "logging.getLogger('idp_common.bedrock').setLevel(logging.INFO)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-CriteriaValidation-Test'\n",
    "os.environ['AWS_REGION'] = boto3.session.Session().region_name or 'us-east-1'\n",
    "\n",
    "# Get AWS account ID for unique bucket names\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = os.environ['AWS_REGION']\n",
    "\n",
    "# Use bucket names from config, but make them unique per account/region\n",
    "user_history_bucket = f\"{config_data['request_bucket']}-{account_id}-{region}\"\n",
    "criteria_bucket = f\"{config_data['criteria_bucket']}-{account_id}-{region}\"\n",
    "output_bucket = f\"{config_data['output_bucket']}-{account_id}-{region}\"\n",
    "\n",
    "print(\"\\nEnvironment setup:\")\n",
    "print(f\"AWS_REGION: {os.environ.get('AWS_REGION')}\")\n",
    "print(f\"User History bucket: {user_history_bucket}\")\n",
    "print(f\"Criteria bucket: {criteria_bucket}\")\n",
    "print(f\"Output bucket: {output_bucket}\")\n",
    "print(\"\\n✅ Async event loop patched for Jupyter compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up S3 Buckets and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Function to create bucket if it doesn't exist\n",
    "def ensure_bucket_exists(bucket_name):\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            if region == 'us-east-1':\n",
    "                s3_client.create_bucket(Bucket=bucket_name)\n",
    "            else:\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "            print(f\"Created bucket: {bucket_name}\")\n",
    "            \n",
    "            # Wait for bucket to be accessible\n",
    "            waiter = s3_client.get_waiter('bucket_exists')\n",
    "            waiter.wait(Bucket=bucket_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket {bucket_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Ensure buckets exist (only user history and output buckets needed)\n",
    "ensure_bucket_exists(user_history_bucket)\n",
    "ensure_bucket_exists(output_bucket)\n",
    "print(\"\\n✅ S3 buckets configured (criteria bucket not needed - using config.yaml)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Sample User History Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample user history\n",
    "sample_user_history = \"\"\"Patient: John Doe\n",
    "Date: 2024-01-15\n",
    "\n",
    "Medical History:\n",
    "The patient has been diagnosed with rheumatoid arthritis (RA) and has failed treatment with methotrexate and two TNF inhibitors. \n",
    "The treating physician, Dr. Sarah Johnson, has recommended starting immunotherapy with infliximab.\n",
    "\n",
    "Treatment Plan:\n",
    "- Infliximab will be administered at the infusion center under direct supervision of trained medical staff\n",
    "- The facility is equipped with emergency response equipment including epinephrine for anaphylaxis treatment\n",
    "- Initial dose: 3 mg/kg at 0, 2, and 6 weeks, then every 8 weeks\n",
    "- Pre-medication with antihistamines and corticosteroids as per protocol\n",
    "\n",
    "Facility Information:\n",
    "The treatment will be provided at Memorial Hospital Infusion Center, which has 24/7 emergency support and trained nursing staff.\n",
    "\n",
    "Additional Clinical Information:\n",
    "- Patient has been screened for contraindications including tuberculosis and hepatitis B\n",
    "- Baseline laboratory values are within normal limits\n",
    "- Patient education has been completed regarding potential side effects and monitoring requirements\n",
    "- Emergency response protocols are in place with trained nursing staff available 24/7\n",
    "\"\"\"\n",
    "\n",
    "# Upload sample data\n",
    "request_id = \"TEST-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "request_prefix = config_data['request_history_prefix']\n",
    "\n",
    "# Upload user history\n",
    "user_history_key = f\"{request_prefix}-{request_id}/extracted_text/patient_history.txt\"\n",
    "s3_client.put_object(\n",
    "    Bucket=user_history_bucket,\n",
    "    Key=user_history_key,\n",
    "    Body=sample_user_history.encode('utf-8')\n",
    ")\n",
    "print(f\"✅ Uploaded user history to: s3://{user_history_bucket}/{user_history_key}\")\n",
    "print(f\"Request ID: {request_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Criteria Validation Service (Using Config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract configuration from loaded YAML and map to service expectations\n",
    "criteria_config = config_data['criteria_validation']\n",
    "\n",
    "validation_config = {\n",
    "    # Model configuration from config.yaml\n",
    "    \"model_id\": criteria_config['model'],\n",
    "    \"temperature\": criteria_config['temperature'],\n",
    "    \"top_k\": criteria_config['top_k'],\n",
    "    \"top_p\": criteria_config['top_p'],\n",
    "    \"max_tokens\": criteria_config['max_tokens'],\n",
    "    \n",
    "    # Bucket configuration\n",
    "    \"request_bucket\": user_history_bucket,\n",
    "    \"request_history_prefix\": request_prefix,\n",
    "    \"criteria_bucket\": criteria_bucket,  # Not used, but kept for compatibility\n",
    "    \"output_bucket\": output_bucket,\n",
    "    \n",
    "    # Criteria types from config.yaml\n",
    "    \"criteria_types\": config_data.get('criteria_types', ['administration_requirements']),\n",
    "    \n",
    "    # Prompts and options from config.yaml\n",
    "    \"system_prompt\": criteria_config['system_prompt'],\n",
    "    \"task_prompt\": criteria_config['task_prompt'],\n",
    "    \"recommendation_options\": criteria_config['criteria']['recommendation_options'],\n",
    "    \n",
    "    # Async processing configuration from config.yaml\n",
    "    \"criteria_validation\": {\n",
    "        \"semaphore\": criteria_config['semaphore'],\n",
    "        \"max_chunk_size\": criteria_config['max_chunk_size'],\n",
    "        \"token_size\": criteria_config['token_size'],\n",
    "        \"overlap_percentage\": criteria_config['overlap_percentage'],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration extracted from config.yaml:\")\n",
    "print(f\"   Model: {validation_config['model_id']}\")\n",
    "print(f\"   Criteria types: {validation_config['criteria_types']}\")\n",
    "print(f\"   Semaphore: {validation_config['criteria_validation']['semaphore']}\")\n",
    "\n",
    "# Display criteria from config for verification\n",
    "print(\"\\n📋 Available criteria from config.yaml:\")\n",
    "for criteria_type in validation_config['criteria_types']:\n",
    "    if criteria_type in criteria_config['criteria']:\n",
    "        questions = criteria_config['criteria'][criteria_type]\n",
    "        print(f\"\\n{criteria_type.replace('_', ' ').title()}:\")\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"  {i}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload Criteria from Config to S3 (For Service Compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the current service expects criteria from S3, we'll upload them from config\n",
    "# This is a temporary step until the service is updated to read from config directly\n",
    "\n",
    "# Ensure criteria bucket exists\n",
    "ensure_bucket_exists(criteria_bucket)\n",
    "\n",
    "# Upload each criteria type from config.yaml to S3\n",
    "criteria_from_config = criteria_config['criteria']\n",
    "\n",
    "for criteria_type in validation_config['criteria_types']:\n",
    "    if criteria_type in criteria_from_config:\n",
    "        # Create criteria data in format expected by service\n",
    "        criteria_data = {\n",
    "            \"criteria\": criteria_from_config[criteria_type]\n",
    "        }\n",
    "        \n",
    "        # Upload to S3\n",
    "        criteria_key = f\"{criteria_type}.json\"\n",
    "        s3_client.put_object(\n",
    "            Bucket=criteria_bucket,\n",
    "            Key=criteria_key,\n",
    "            Body=json.dumps(criteria_data, indent=2).encode('utf-8')\n",
    "        )\n",
    "        print(f\"✅ Uploaded {criteria_type} criteria to: s3://{criteria_bucket}/{criteria_key}\")\n",
    "\n",
    "print(\"\\n✅ All criteria from config.yaml uploaded to S3 for service compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Criteria Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the service\n",
    "criteria_service = CriteriaValidationService(\n",
    "    region=region,\n",
    "    config=validation_config\n",
    ")\n",
    "\n",
    "print(f\"🚀 Starting validation for request: {request_id}\")\n",
    "print(f\"Processing {len(validation_config['criteria_types'])} criteria types...\")\n",
    "print(\"This may take a few moments...\")\n",
    "\n",
    "# Run validation\n",
    "start_time = time.time()\n",
    "result = criteria_service.validate_request(\n",
    "    request_id=request_id,\n",
    "    config=validation_config\n",
    ")\n",
    "validation_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✅ Validation completed in {validation_time:.2f} seconds\")\n",
    "print(f\"Request ID: {result.request_id}\")\n",
    "print(f\"Criteria Type: {result.criteria_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display validation responses\n",
    "print(\"\\n=== 📊 Validation Results ===\")\n",
    "\n",
    "# Helper function to parse S3 URIs\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "# Read results from S3\n",
    "if result.metadata and 'output_uris' in result.metadata:\n",
    "    for output_uri in result.metadata['output_uris']:\n",
    "        print(f\"\\n📄 Reading results from: {output_uri}\")\n",
    "        \n",
    "        # Parse S3 URI and read content\n",
    "        bucket, key = parse_s3_uri(output_uri)\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        responses = json.loads(content)\n",
    "        \n",
    "        # Display each validation response\n",
    "        for idx, response_item in enumerate(responses):\n",
    "            print(f\"\\n--- 📋 Criteria {idx + 1} ---\")\n",
    "            print(f\"❓ Question: {response_item.get('question', 'N/A')}\")\n",
    "            \n",
    "            recommendation = response_item.get('Recommendation', 'N/A')\n",
    "            if recommendation == 'Pass':\n",
    "                print(f\"✅ Recommendation: {recommendation}\")\n",
    "            elif recommendation == 'Fail':\n",
    "                print(f\"❌ Recommendation: {recommendation}\")\n",
    "            else:\n",
    "                print(f\"❓ Recommendation: {recommendation}\")\n",
    "            \n",
    "            print(f\"📝 Reasoning: {response_item.get('Reasoning', 'N/A')}\")\n",
    "            print(f\"📁 Source Files: {response_item.get('source_file', [])}\")\n",
    "else:\n",
    "    print(\"❌ No validation results found in metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Metering and Cost Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metering information with support for nested model-specific structure\n",
    "print(\"\\n=== 💰 Token Usage ===\")\n",
    "if result.metering:\n",
    "    # First try the old flat structure for backward compatibility\n",
    "    if 'total_input_tokens' in result.metering:\n",
    "        print(f\"📥 Total Input Tokens: {result.metering.get('total_input_tokens', 0):,}\")\n",
    "        print(f\"📤 Total Output Tokens: {result.metering.get('total_output_tokens', 0):,}\")\n",
    "    else:\n",
    "        # Handle new nested structure: {model_key: {inputTokens: X, outputTokens: Y, totalTokens: Z}}\n",
    "        total_input_tokens = 0\n",
    "        total_output_tokens = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        print(\"\\n📊 Per-Model Token Usage:\")\n",
    "        for model_key, usage in result.metering.items():\n",
    "            if isinstance(usage, dict) and ('inputTokens' in usage or 'outputTokens' in usage):\n",
    "                input_tokens = usage.get('inputTokens', 0)\n",
    "                output_tokens = usage.get('outputTokens', 0)\n",
    "                model_total = usage.get('totalTokens', input_tokens + output_tokens)\n",
    "                \n",
    "                # Extract model name from the key for cleaner display\n",
    "                model_name = model_key.split('/')[-1] if '/' in model_key else model_key\n",
    "                print(f\"  🤖 {model_name}:\")\n",
    "                print(f\"    📥 Input Tokens: {input_tokens:,}\")\n",
    "                print(f\"    📤 Output Tokens: {output_tokens:,}\")\n",
    "                print(f\"    📊 Total Tokens: {model_total:,}\")\n",
    "                \n",
    "                # Add to totals\n",
    "                total_input_tokens += input_tokens\n",
    "                total_output_tokens += output_tokens\n",
    "                total_tokens += model_total\n",
    "        \n",
    "        print(f\"\\n📈 Total Across All Models:\")\n",
    "        print(f\"  📥 Total Input Tokens: {total_input_tokens:,}\")\n",
    "        print(f\"  📤 Total Output Tokens: {total_output_tokens:,}\")\n",
    "        print(f\"  📊 Grand Total Tokens: {total_tokens:,}\")\n",
    "    \n",
    "    # Display per-criteria usage if available (legacy structure)\n",
    "    criteria_usage = result.metering.get('criteria_usage', {})\n",
    "    if criteria_usage:\n",
    "        print(\"\\n📋 Per-Criteria Usage:\")\n",
    "        for criteria_type, usage in criteria_usage.items():\n",
    "            print(f\"  📄 {criteria_type}:\")\n",
    "            print(f\"    📥 Input Tokens: {usage.get('input_tokens', 0):,}\")\n",
    "            print(f\"    📤 Output Tokens: {usage.get('output_tokens', 0):,}\")\n",
    "else:\n",
    "    print(\"❌ No metering data available\")\n",
    "\n",
    "# Display timing information\n",
    "print(\"\\n=== ⏱️ Timing Information ===\")\n",
    "if result.metadata and 'timing' in result.metadata:\n",
    "    timing = result.metadata['timing']\n",
    "    print(f\"🕐 Total Duration: {timing.get('total_duration', 0):.2f} seconds\")\n",
    "    \n",
    "    # Display per-criteria timing\n",
    "    criteria_timing = timing.get('criteria_processing_time', [])\n",
    "    if criteria_timing:\n",
    "        print(\"\\n📊 Per-Criteria Processing Time:\")\n",
    "        for item in criteria_timing:\n",
    "            print(f\"  📋 {item['criteria_type']}: {item['duration']:.2f} seconds\")\n",
    "else:\n",
    "    print(\"❌ No timing data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete objects in a bucket\n",
    "def delete_bucket_objects(bucket_name):\n",
    "    try:\n",
    "        # List all objects in the bucket\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            delete_keys = {'Objects': [{'Key': obj['Key']} for obj in response['Contents']]}\n",
    "            s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\n",
    "            print(f\"🗑️ Deleted all objects in bucket {bucket_name}\")\n",
    "        else:\n",
    "            print(f\"📭 Bucket {bucket_name} is already empty\")\n",
    "            \n",
    "        # Delete bucket\n",
    "        s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        print(f\"🗑️ Deleted bucket {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cleaning up bucket {bucket_name}: {str(e)}\")\n",
    "\n",
    "# Uncomment the following lines to delete the buckets\n",
    "# print(\"🧹 Cleaning up resources...\")\n",
    "# delete_bucket_objects(user_history_bucket)\n",
    "# delete_bucket_objects(criteria_bucket)\n",
    "# delete_bucket_objects(output_bucket)\n",
    "# print(\"✅ Cleanup complete\")\n",
    "\n",
    "print(\"\\n✅ Notebook completed successfully!\")\n",
    "print(\"💡 Uncomment the cleanup section above to delete the test S3 buckets.\")\n",
    "print(f\"🎯 Configuration successfully migrated from pattern-4 to pattern-2\")\n",
    "print(f\"📋 All criteria now sourced from: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook now demonstrates the **updated** criteria validation module with **centralized configuration**:\n",
    "\n",
    "### ✅ New Features:\n",
    "1. **Centralized Config** - All configuration now loaded from `config.yaml`\n",
    "2. **Config-Driven Criteria** - Criteria questions defined in config, not hardcoded\n",
    "3. **Pattern-2 Integration** - Config moved from pattern-4 to pattern-2\n",
    "4. **YAML Support** - Added pyyaml dependency for config loading\n",
    "\n",
    "### 🔧 Technical Capabilities:\n",
    "1. **Async Processing** - Concurrent evaluation of multiple criteria questions\n",
    "2. **Rate Limiting** - Built-in semaphore control for API rate limits\n",
    "3. **Chunking** - Automatic text chunking for large documents\n",
    "4. **Cost Tracking** - Comprehensive token usage and metering\n",
    "5. **Pydantic Validation** - Strong data validation for inputs/outputs\n",
    "6. **S3 Integration** - Seamless reading/writing of validation data\n",
    "7. **Jupyter Compatibility** - Fixed async event loop conflicts with nest_asyncio\n",
    "\n",
    "### 📈 Key Benefits:\n",
    "- **Maintainability** - Single source of truth for all configuration\n",
    "- **Scalability** - Process multiple criteria types concurrently\n",
    "- **Reliability** - Built-in error handling and retry logic\n",
    "- **Consistency** - Uses common bedrock client for standardized LLM interactions\n",
    "- **Flexibility** - Easy to modify criteria without code changes\n",
    "- **Traceability** - Complete audit trail with source file citations\n",
    "\n",
    "### 🎯 Migration Summary:\n",
    "- ✅ Moved config from `pattern-4/criteria-validation/` to `pattern-2/criteria-validation/`\n",
    "- ✅ Added criteria definitions directly in config.yaml under `criteria_validation.criteria`\n",
    "- ✅ Updated notebook to load config from YAML instead of inline definitions\n",
    "- ✅ Eliminated hardcoded criteria and recommendation options\n",
    "- ✅ Maintained backward compatibility with existing service architecture\n",
    "\n",
    "The module is designed for healthcare/insurance prior authorization validation but can be adapted for other business rule validation use cases by simply updating the config.yaml file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiic-idp-accelerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
