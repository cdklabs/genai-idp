# Evaluation Service Configuration
evaluation:
  llm_method:
    top_p: '0.1'
    max_tokens: '4096'
    top_k: '5'
    task_prompt: >-
      I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.


      For the attribute named "{ATTRIBUTE_NAME}" described as "{ATTRIBUTE_DESCRIPTION}":

      - Expected value: {EXPECTED_VALUE}

      - Actual value: {ACTUAL_VALUE}


      Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?

      Provide your assessment as a JSON with three fields:

      - "match": boolean (true if they match, false if not)

      - "score": number between 0 and 1 representing the confidence/similarity score

      - "reason": brief explanation of your decision


      Respond ONLY with the JSON and nothing else. Here's the exact format:

      {
        "match": true or false,
        "score": 0.0 to 1.0,
        "reason": "Your explanation here"
      }
    temperature: '0.0'
    model: us.anthropic.claude-3-haiku-20240307-v1:0
    system_prompt: >-
      You are an evaluator that helps determine if the predicted and expected values match for document attribute extraction. You will consider the context and meaning rather than just exact string matching.

