/*
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
SPDX-License-Identifier: Apache-2.0
*/

import * as path from "path";
import { bedrock } from "@cdklabs/generative-ai-cdk-constructs";
import {
  IGuardrail,
  IInvokable,
  IKnowledgeBase,
} from "@cdklabs/generative-ai-cdk-constructs/lib/cdk-lib/bedrock";
import * as cdk from "aws-cdk-lib";
import * as appsync from "aws-cdk-lib/aws-appsync";
import * as kms from "aws-cdk-lib/aws-kms";
import * as logs from "aws-cdk-lib/aws-logs";
import { IBucket } from "aws-cdk-lib/aws-s3";
import * as secretsmanager from "aws-cdk-lib/aws-secretsmanager";
import { IQueue } from "aws-cdk-lib/aws-sqs";
import * as stepfunctions from "aws-cdk-lib/aws-stepfunctions";
import { Construct } from "constructs";
import { AgentAnalytics } from "../agent-analytics";
import { LogLevel } from "../log-level";
import { ProcessingEnvironmentApiBaseProps } from "./processing-environment-api-base-props";
import { IConfigurationTable } from "../configuration-table";
import { IDocumentDiscovery } from "../document-discovery";
import { VpcConfiguration } from "../vpc-configuration";
import * as functions from "./functions";
import { IReportingEnvironment } from "../reporting/reporting-environment";
import { ITrackingTable } from "../tracking-table";

/**
 * Interface for the document processing environment API.
 * Provides GraphQL API capabilities for monitoring and managing document processing.
 */
export interface IProcessingEnvironmentApi extends appsync.IGraphqlApi {
  /**
   * The URL endpoint for the GraphQL API.
   * Used by client applications to interact with the document processing system.
   */
  readonly graphqlUrl: string;
}

/**
 * Properties for configuring the ProcessingEnvironmentApi construct.
 * Extends the base properties with additional settings specific to document processing.
 */
export interface ProcessingEnvironmentApiProps
  extends ProcessingEnvironmentApiBaseProps {
  /**
   * The S3 bucket where source documents to be processed are stored.
   * This bucket is monitored for new document uploads to trigger processing.
   */
  readonly inputBucket: IBucket;

  /**
   * The S3 bucket where processed documents and extraction results are stored.
   * Contains the structured data output and processing artifacts.
   */
  readonly outputBucket: IBucket;

  /**
   * The DynamoDB table that tracks document processing status and metadata.
   * Stores information about documents being processed, including status and results.
   */
  readonly trackingTable: ITrackingTable;

  /**
   * The DynamoDB table that stores configuration settings.
   * Contains document schemas, extraction parameters, and other system-wide settings.
   */
  readonly configurationTable: IConfigurationTable;

  /**
   * Optional KMS key used for encrypting sensitive data in the processing environment.
   * When provided, ensures that document content and metadata are encrypted at rest.
   */
  readonly encryptionKey?: kms.IKey;

  /**
   * The retention period for CloudWatch logs generated by document processing components.
   * Controls how long system logs are kept for troubleshooting and auditing.
   */
  readonly logRetention?: logs.RetentionDays;

  /**
   * The log level for document processing components.
   * Controls the verbosity of logs generated during document processing.
   */
  readonly logLevel?: LogLevel;

  /**
   * Optional VPC configuration for document processing components.
   * When provided, deploys processing components within a VPC with specified settings.
   */
  readonly vpcConfiguration?: VpcConfiguration;

  /**
   * Optional S3 bucket name for storing evaluation baseline documents.
   * Used for comparing extraction results against known correct values
   * to measure accuracy and evaluate model performance.
   */
  readonly evaluationBaselineBucket?: IBucket;

  /**
   * Optional invokable model to use for knowledge base queries.
   * Can be a Bedrock foundation model, Bedrock inference profile, or custom model.
   * Enables natural language querying of processed documents when a knowledge base is configured.
   *
   * @default bedrock.BedrockFoundationModel.AMAZON_NOVA_PRO_V1_0
   */
  readonly knowledgeBaseModel?: bedrock.IInvokable;

  /**
   * Optional knowledge base identifier for document querying capabilities.
   * When provided, enables natural language querying of processed documents
   * using the specified Amazon Bedrock knowledge base.
   */
  readonly knowledgeBase?: bedrock.IKnowledgeBase;

  /**
   * Optional Bedrock guardrail to apply to model interactions.
   * Helps ensure model outputs adhere to content policies and guidelines
   * by filtering inappropriate content and enforcing usage policies.
   */
  readonly knowledgeBaseGuardrail?: bedrock.IGuardrail;

  /**
   * Optional Step Functions state machine for document processing workflow.
   * When provided, enables querying of execution details and step-by-step
   * processing status through the GraphQL API.
   */
  readonly stateMachine?: stepfunctions.IStateMachine;

  /**
   * Optional document discovery for automated document analysis.
   * When provided, enables document discovery capabilities including
   * automated configuration generation and document structure analysis.
   */
  readonly documentDiscovery?: IDocumentDiscovery;

  /**
   * The S3 bucket for working files during document processing.
   * Used for temporary storage of intermediate processing results.
   */
  readonly workingBucket?: IBucket;

  /**
   * The SQS queue for document processing requests.
   * Used to queue documents for processing and manage workflow execution.
   */
  readonly documentQueue?: IQueue;

  /**
   * Data retention period in days for processed documents.
   * Controls how long document data is kept in the system.
   */
  readonly dataRetentionInDays?: number;
}

/**
 * A construct that provides a GraphQL API for tracking and managing document processing.
 *
 * The ProcessingEnvironmentApi creates an AppSync GraphQL API with resolvers for:
 * - Querying document status and metadata
 * - Managing document processing (delete, reprocess)
 * - Accessing document contents and extraction results
 * - Uploading new documents for processing
 * - Copying documents to baseline for evaluation
 * - Querying document knowledge base (if configured)
 *
 * It integrates with the processing environment's resources including DynamoDB tables,
 * S3 buckets, and optional knowledge base to provide a comprehensive interface for
 * monitoring and managing the document processing workflow.
 */
/**
 * A construct that provides a GraphQL API for tracking and managing document processing.
 *
 * The ProcessingEnvironmentApi creates an AppSync GraphQL API with resolvers for:
 * - Querying document status and metadata
 * - Managing document processing (delete, reprocess)
 * - Accessing document contents and extraction results
 * - Uploading new documents for processing
 * - Copying documents to baseline for evaluation
 * - Querying document knowledge base (if configured)
 *
 * It integrates with the processing environment's resources including DynamoDB tables,
 * S3 buckets, and optional knowledge base to provide a comprehensive interface for
 * monitoring and managing the document processing workflow.
 */
export class ProcessingEnvironmentApi
  extends appsync.GraphqlApi
  implements IProcessingEnvironmentApi
{
  private readonly _logLevel?: LogLevel;
  private readonly _encryptionKey?: kms.IKey;
  private readonly _logRetention?: logs.RetentionDays;
  private readonly _vpcConfiguration?: VpcConfiguration;
  private readonly _configurationTable: IConfigurationTable;
  private readonly outputBucket: IBucket;
  private _agentAnalytics?: AgentAnalytics;

  /**
   * Creates a new ProcessingEnvironmentApi.
   *
   * @param scope The construct scope
   * @param id The construct ID
   * @param props Configuration properties for the API
   */
  constructor(
    scope: Construct,
    id: string,
    props: ProcessingEnvironmentApiProps,
  ) {
    super(scope, id, {
      ...props,
      name:
        props.name ||
        cdk.Lazy.string({
          produce: () => cdk.Names.uniqueResourceName(this, { maxLength: 128 }),
        }),
      definition: appsync.Definition.fromFile(
        path.join(
          __dirname,
          "..",
          "..",
          "assets",
          "appsync",
          "env-api",
          "schema.graphql",
        ),
      ),
    });

    // Store configuration for later use in add* methods
    this._logLevel = props.logLevel;
    this._encryptionKey = props.encryptionKey;
    this._logRetention = props.logRetention;
    this._vpcConfiguration = props.vpcConfiguration;
    this._configurationTable = props.configurationTable;
    this.outputBucket = props.outputBucket;

    // Add file contents resolver
    const getFileContentsDataSource = this.addGetFileContentsDataSource(
      props.inputBucket,
      props.outputBucket,
      props.encryptionKey,
      props.vpcConfiguration,
    );
    this.createGetFileContentsQueryResolver(getFileContentsDataSource);

    // Add document upload and reprocess resolvers
    const uploadDocumentDataSource = this.addUploadDocumentDataSource(
      props.inputBucket,
      props.outputBucket,
      props.evaluationBaselineBucket,
      props.encryptionKey,
      props.logRetention,
      props.vpcConfiguration,
    );
    this.createUploadDocumentMutationResolver(uploadDocumentDataSource);

    const reprocessDocumentDataSource = this.addReprocessDocumentDataSource(
      props.inputBucket,
      props.encryptionKey,
      props.logRetention,
      props.vpcConfiguration,
    );

    this.createReprocessDocumentMutationResolver(reprocessDocumentDataSource);

    // Add process changes resolver if required properties are available
    if (
      props.workingBucket &&
      props.documentQueue &&
      props.dataRetentionInDays !== undefined
    ) {
      const processChangesDataSource = this.addProcessChangesDataSource(
        props.trackingTable,
        props.documentQueue,
        props.workingBucket,
        props.inputBucket,
        props.outputBucket,
        props.dataRetentionInDays,
        props.encryptionKey,
        props.logRetention,
        props.vpcConfiguration,
      );
      this.createProcessChangesMutationResolver(processChangesDataSource);
    }

    // Add optional components using modular methods
    if (props.evaluationBaselineBucket) {
      this.addEvaluation(props.evaluationBaselineBucket);
    }

    if (props.knowledgeBase && props.knowledgeBaseModel) {
      this.addKnowledgeBase(
        props.knowledgeBase,
        props.knowledgeBaseModel,
        props.knowledgeBaseGuardrail,
      );
    }

    if (props.stateMachine) {
      this.addStateMachine(props.stateMachine);
    }

    // Add core functionality
    this.addTrackingTable(
      props.trackingTable,
      props.inputBucket,
      props.outputBucket,
    );
    this.addConfigurationTable(props.configurationTable);

    // Add optional Discovery functionality
    if (props.documentDiscovery) {
      this.addDocumentDiscovery(props.documentDiscovery);
    }
  }

  /**
   * Add Step Functions resolvers and monitoring for the GraphQL API.
   *
   * This method adds Step Functions execution monitoring capabilities to the API,
   * including query resolvers, mutation resolvers, and automatic subscription publishing.
   * It can be called after the API has been created to add Step Functions functionality
   * for the specified state machine.
   *
   * @example
   * // Add state machine monitoring after API creation
   * api.addStateMachine(myStateMachine);
   *
   * @param stateMachine The Step Functions state machine to monitor
   */
  public addStateMachine(stateMachine: stepfunctions.IStateMachine): void {
    // Add Step Function execution query resolver
    const getStepFunctionExecutionDataSource =
      this.addGetStepFunctionExecutionDataSource(
        stateMachine,
        this._encryptionKey,
        this._logRetention,
        this._vpcConfiguration,
      );
    this.createGetStepFunctionExecutionQueryResolver(
      getStepFunctionExecutionDataSource,
    );
  }

  /**
   * Add knowledge base querying capabilities to the GraphQL API.
   *
   * This method adds natural language querying functionality for processed documents
   * using Amazon Bedrock knowledge base. It creates the necessary resolvers and
   * data sources to enable document querying through the GraphQL API.
   *
   * @example
   * // Add knowledge base functionality after API creation
   * api.addKnowledgeBase(
   *   myKnowledgeBase,
   *   bedrock.BedrockFoundationModel.AMAZON_NOVA_PRO_V1_0,
   *   myGuardrail
   * );
   *
   * @param knowledgeBase The Amazon Bedrock knowledge base for document querying
   * @param knowledgeBaseModel The invokable model to use for knowledge base queries
   * @param knowledgeBaseGuardrail Optional Bedrock guardrail to apply to model interactions
   */
  public addKnowledgeBase(
    knowledgeBase: bedrock.IKnowledgeBase,
    knowledgeBaseModel: bedrock.IInvokable,
    knowledgeBaseGuardrail?: bedrock.IGuardrail,
  ): void {
    const queryKnowledgeBaseDataSource = this.addQueryKnowledgeBaseDataSource(
      knowledgeBase,
      knowledgeBaseModel,
      knowledgeBaseGuardrail,
      this._logLevel,
      this._encryptionKey,
      this._logRetention,
      this._vpcConfiguration,
    );

    this.createQueryKnowledgeBaseQueryResolver(queryKnowledgeBaseDataSource);
  }

  /**
   * Add evaluation capabilities to the GraphQL API.
   *
   * This method adds document evaluation functionality, including the ability
   * to copy documents to a baseline bucket for evaluation purposes.
   * It creates the necessary resolvers and data sources for evaluation workflows.
   *
   * @example
   * // Add evaluation functionality after API creation
   * api.addEvaluation(myEvaluationBaselineBucket);
   *
   * @param evaluationBaselineBucket The S3 bucket for storing evaluation baseline documents
   */
  public addEvaluation(evaluationBaselineBucket: IBucket): void {
    const copyToBaselineDataSource = this.addCopyToBaselineDataSource(
      evaluationBaselineBucket,
      this.outputBucket,
      this._encryptionKey,
      this._logRetention,
      this._vpcConfiguration,
    );
    this.createCopyToBaselineMutationResolver(copyToBaselineDataSource);
  }

  /**
   * Add tracking table data sources and resolvers to the GraphQL API.
   *
   * This method adds all tracking table related functionality including:
   * - Document creation and management
   * - Document status tracking
   * - Document listing and querying
   * - Document metadata management
   * - Document deletion (from tracking table and S3 buckets)
   *
   * @example
   * // Add tracking table functionality after API creation
   * api.addTrackingTable(myTrackingTable, inputBucket, outputBucket);
   *
   * @param trackingTable The DynamoDB table that tracks document processing status and metadata
   * @param inputBucket The S3 bucket where source documents are stored
   * @param outputBucket The S3 bucket where processed documents are stored
   */
  public addTrackingTable(
    trackingTable: ITrackingTable,
    inputBucket: IBucket,
    outputBucket: IBucket,
  ): void {
    // Add delete document functionality (depends on tracking table)
    const deleteDocumentDataSource = this.addDeleteDocumentDataSource(
      trackingTable,
      inputBucket,
      outputBucket,
      this._encryptionKey,
      this._logRetention,
      this._vpcConfiguration,
    );
    this.createDeleteDocumentMutationResolver(deleteDocumentDataSource);

    // Add core tracking table functionality
    this.addTrackingTableDataSourceAndResolvers(trackingTable);
  }

  /**
   * Add configuration table data sources and resolvers to the GraphQL API.
   *
   * This method adds configuration management functionality including:
   * - Querying configuration settings
   * - Updating configuration parameters
   * - Managing document schemas and extraction parameters
   *
   * @example
   * // Add configuration table functionality after API creation
   * api.addConfigurationTable(myConfigurationTable);
   *
   * @param configurationTable The DynamoDB table that stores configuration settings
   */
  public addConfigurationTable(configurationTable: IConfigurationTable): void {
    this.addConfigurationResolver(
      configurationTable,
      this._encryptionKey,
      this._logRetention,
      this._vpcConfiguration,
    );
  }

  /**
   * Add Agent Analytics capabilities to the GraphQL API.
   *
   * This method adds AI-powered analytics functionality that enables natural language
   * querying of processed document data. It creates the necessary resolvers and data sources
   * for agent analytics workflows including database discovery, SQL query generation,
   * and interactive visualizations.
   *
   * @example
   * // Add agent analytics after API creation
   * api.addAgentAnalytics(
   *   trackingTable,
   *   myAnalyticsModel,
   *   reportingDatabase,
   *   athenaBucket
   * );
   *
   * @param trackingTable The DynamoDB table that tracks document processing status
   * @param model The foundation model or inference profile for analytics queries
   * @param reportingEnvironment The reporting environment that the analytics will be run for
   * @param externalMcpAgentsSecret Optional Secrets Manager secret for external MCP agents
   * @param guardrail Optional Bedrock guardrail for content filtering
   */
  public addAgentAnalytics(
    trackingTable: ITrackingTable,
    model: bedrock.IInvokable,
    reportingEnvironment: IReportingEnvironment,
    externalMcpAgentsSecret?: secretsmanager.ISecret,
    guardrail?: bedrock.IGuardrail,
  ): void {
    this._agentAnalytics = new AgentAnalytics(this, "AgentAnalytics", {
      trackingTable,
      configurationTable: this._configurationTable,
      model,
      metricNamespace: "GenAI-IDP",
      appSyncApiUrl: this.graphqlUrl,
      reportingEnvironment,
      encryptionKey: this._encryptionKey,
      logLevel: this._logLevel,
      logRetention: this._logRetention,
      externalMcpAgentsSecret,
      guardrail,
    });

    // Add data sources and resolvers for agent analytics
    this.addAgentAnalyticsDataSources();
  }

  /**
   * Add Chat with Document capabilities to the GraphQL API.
   *
   * This method adds natural language conversation functionality about processed documents
   * by combining document context from the knowledge base with conversational AI.
   * It maintains conversation history and provides contextual responses.
   *
   * @example
   * // Add chat with document after API creation
   * api.addChatWithDocument(
   *   knowledgeBase,
   *   chatModel,
   *   myGuardrail
   * );
   *
   * @param knowledgeBase The Bedrock knowledge base for document context
   * @param chatModel The invokable model for chat functionality
   * @param guardrail Optional Bedrock guardrail for content filtering
   */
  public addChatWithDocument(
    knowledgeBase: bedrock.IKnowledgeBase,
    chatModel: bedrock.IInvokable,
    guardrail?: bedrock.IGuardrail,
  ): void {
    const chatWithDocumentDataSource = this.addChatWithDocumentDataSource(
      knowledgeBase,
      chatModel,
      guardrail,
      this._logLevel,
      this._encryptionKey,
      this._logRetention,
      this._vpcConfiguration,
    );

    this.createChatWithDocumentQueryResolver(chatWithDocumentDataSource);
  }

  private addTrackingTableDataSourceAndResolvers(
    trackingTable: ITrackingTable,
  ) {
    const trackingTableDataSource = this.addDynamoDbDataSource(
      "TrackingTableDataSource",
      trackingTable,
    );

    // Create the createDocument resolver
    trackingTableDataSource.createResolver("CreateDocumentResolver", {
      typeName: "Mutation",
      fieldName: "createDocument",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
          #set( $PK = "doc#\${ctx.args.input.ObjectKey}" )
  
          #set( $shardsInDay = 6 )
          #set( $shardDivider = 24 / $shardsInDay )
          #set( $Integer = 0 )
          #set( $now = $ctx.args.input.QueuedTime )
          #set( $date = $now.substring(0, 10) )
          #set( $hourString = $now.substring(11, 13) )
          #set( $hour = $Integer.parseInt($hourString) )
          #set( $hourShard = $hour / $shardDivider )
          #set( $shardPad = $date.format("%02d", $hourShard) )
          #set( $listPk = "list#\${date}#s#\${shardPad}" )
          #set( $listSk = "ts#\${now}#id#\${ctx.args.input.ObjectKey}" )
  
          {
            "version" : "2018-05-29",
            "operation" : "TransactWriteItems",
            "transactItems": [
              {
                "table": "${trackingTable.tableName}",
                "operation": "PutItem",
                "key" : {
                  "PK": $util.dynamodb.toDynamoDBJson($PK),
                  "SK": $util.dynamodb.toDynamoDBJson("none"),
                },
                "attributeValues": $util.dynamodb.toMapValuesJson($ctx.args.input),
              },
              {
                "table": "${trackingTable.tableName}",
                "operation": "PutItem",
                "key" : {
                  "PK": $util.dynamodb.toDynamoDBJson($listPk),
                  "SK": $util.dynamodb.toDynamoDBJson($listSk),
                },
                "attributeValues": {
                  "ObjectKey": $util.dynamodb.toDynamoDBJson($ctx.args.input.ObjectKey),
                  "QueuedTime": $util.dynamodb.toDynamoDBJson($ctx.args.input.QueuedTime),
                  "ExpiresAfter": $util.dynamodb.toDynamoDBJson($ctx.args.input.ExpiresAfter),
                },
              },
            ],
          }`),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`
          #if($ctx.error)
            $util.error($ctx.error.message, $ctx.error.type)
          #end
          $util.toJson({"ObjectKey": $ctx.args.input.ObjectKey})`),
    });

    trackingTableDataSource.createResolver("UpdateDocumentResolver", {
      typeName: "Mutation",
      fieldName: "updateDocument",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
        #set( $PK = "doc#\${ctx.args.input.ObjectKey}" )
        #set( $expNames = {} )
        #set( $expValues = {} )
        #set( $expSet = {} )
        ## Iterate through each argument with values and update expression variables **
        #foreach( $entry in $ctx.args.input.entrySet() )
            ## skip empty values **
            #if( !$util.isNullOrBlank($entry.value)  )
                $util.qr( $expSet.put("#\${entry.key}", ":\${entry.key}") )
                $util.qr( $expNames.put("#\${entry.key}", "\${entry.key}") )
                $util.qr( $expValues.put(":\${entry.key}", $util.dynamodb.toDynamoDB($entry.value)) )
            #end
        #end
        ## Start building the update expression, starting with attributes we're going to SET **
        #set( $expression = "" )
        #if( !$\{expSet.isEmpty()} )
            #set( $expression = "SET" )
            #foreach( $entry in $expSet.entrySet() )
                #set( $expression = "\${expression} \${entry.key} = \${entry.value}" )
                #if ( $foreach.hasNext )
                    #set( $expression = "\${expression}," )
                #end
            #end
        #end
        {
            "version" : "2018-05-29",
            "operation" : "UpdateItem",
            "key" : {
              "PK": $util.dynamodb.toDynamoDBJson($PK),
              "SK": $util.dynamodb.toDynamoDBJson("none"),
            },
            "update" : {
                "expression": "$expression"
                #if( !$\{expNames.isEmpty()} )
                , "expressionNames": $utils.toJson($expNames)
                #end
                #if( !$\{expValues.isEmpty()} )
                , "expressionValues": $utils.toJson($expValues)
                #end
            }
        }`),
      responseMappingTemplate: appsync.MappingTemplate.fromString(
        "$util.toJson($ctx.result)",
      ),
    });

    trackingTableDataSource.createResolver("GetDocumentResolver", {
      typeName: "Query",
      fieldName: "getDocument",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
        #set( $PK = "doc#\${context.arguments.ObjectKey}" )
        {
          "version": "2018-05-29",
          "operation": "GetItem",
          "key" : {
            "PK": $util.dynamodb.toDynamoDBJson($PK),
            "SK": $util.dynamodb.toDynamoDBJson("none"),
          }
        }`),
      responseMappingTemplate: appsync.MappingTemplate.fromString(
        "$util.toJson($ctx.result)",
      ),
    });

    trackingTableDataSource.createResolver("ListDocumentResolver", {
      typeName: "Query",
      fieldName: "listDocuments",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
        {
            "version": "2018-05-29",
            "operation": "Scan",
            "filter": {
                #if($context.arguments.startDateTime && $context.arguments.endDateTime)
                    "expression": "InitialEventTime BETWEEN :startDateTime AND :endDateTime",
                    "expressionValues": {
                        ":startDateTime": { "S": "$context.arguments.startDateTime" },
                        ":endDateTime": { "S": "$context.arguments.endDateTime" }
                    }
                #elseif($context.arguments.startDateTime)
                    "expression": "InitialEventTime >= :startDateTime",
                    "expressionValues": {
                        ":startDateTime": { "S": "$context.arguments.startDateTime" }
                    }
                #elseif($context.arguments.endDateTime)
                    "expression": "InitialEventTime <= :endDateTime",
                    "expressionValues": {
                        ":endDateTime": { "S": "$context.arguments.endDateTime" }
                    }
                #end
            },
            #if($context.prev.result)
                "nextToken": "$context.prev.result.nextToken",
            #end
            "limit": 50,
            "consistentRead": false,
            "select": "ALL_ATTRIBUTES"
        }`),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`
            {
                "Documents": $util.toJson($ctx.result.items),
                "nextToken": $util.toJson($ctx.result.nextToken)
            }`),
    });

    trackingTableDataSource.createResolver("ListDocumentDateHourResolver", {
      typeName: "Query",
      fieldName: "listDocumentsDateHour",
      requestMappingTemplate: appsync.MappingTemplate
        .fromString(`#set( $shardsInDay = 6 )
        #set( $shardDivider = 24 / $shardsInDay )
        #set( $Integer = 0 )
        #set( $now = $util.time.nowISO8601() )
        #set( $hourNow = $Integer.parseInt($now.substring(11, 13)) )
        #set( $date = $util.defaultIfNullOrBlank($ctx.args.date, $now.substring(0, 10)) )
        #set( $hour = $util.defaultIfNull($ctx.args.hour, $hourNow) )
        #if( $hour < 0 || $hour > 23 )
          $util.error("Invalid hour parameter - value should be between 0 and 23")
        #end
        #set( $hourPad = $date.format("%02d", $hour) )
        #set( $hourShard = $hour / $shardDivider )
        #set( $shardPad = $date.format("%02d", $hourShard) )

        #set( $PK = "list#\${date}#s#\${shardPad}" )
        #set( $skPrefix = "ts#\${date}T\${hourPad}" )

        {
          "version" : "2018-05-29",
          "operation" : "Query",
          "query" : {
            "expression": "PK = :PK and begins_with(SK, :prefix)",
            "expressionValues": {
              ":PK": $util.dynamodb.toDynamoDBJson($PK),
              ":prefix": $util.dynamodb.toDynamoDBJson($skPrefix),
            },
          },
        }`),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`{
            "Documents": $util.toJson($ctx.result.items),
            "nextToken": $util.toJson($ctx.result.nextToken)
        }`),
    });

    trackingTableDataSource.createResolver("ListDocumentDateShardResolver", {
      typeName: "Query",
      fieldName: "listDocumentsDateShard",
      requestMappingTemplate: appsync.MappingTemplate
        .fromString(`#set( $shardsInDay = 6 )
        #set( $shardDivider = 24 / $shardsInDay )
        #set( $Integer = 0 )
        #set( $now = $util.time.nowISO8601() )
        #set( $hourNow = $Integer.parseInt($now.substring(11, 13)) )
        #set( $shardNow = $hourNow / $shardDivider )
        #set( $date = $util.defaultIfNullOrBlank($ctx.args.date, $now.substring(0, 10)) )
        #set( $shard = $util.defaultIfNull($ctx.args.shard, $shardNow) )
        #if( $shard >= $shardsInDay )
          $util.error("Invalid shard parameter value - must positive and less than \${shardsInDay}")
        #end
        #set( $hourShard = $hour / $shardDivider )
        #set( $shardPad = $date.format("%02d", $shard) )

        #set( $PK = "list#\${date}#s#\${shardPad}" )

        {
          "version" : "2018-05-29",
          "operation" : "Query",
          "query" : {
            "expression": "PK = :PK",
            "expressionValues": {
              ":PK": $util.dynamodb.toDynamoDBJson($PK),
            }
          }
        }`),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`{
            "Documents": $util.toJson($ctx.result.items),
            "nextToken": $util.toJson($ctx.result.nextToken)
        }`),
    });
  }

  /**
   * Add Get File Contents Data Source to the GraphQL API.
   *
   * This method creates a Lambda data source for retrieving file contents from S3 buckets.
   * The data source can be used to create resolvers that allow clients to fetch
   * document contents from various S3 buckets (input, output, configuration).
   *
   * @param inputBucket The S3 bucket for input documents
   * @param outputBucket The S3 bucket for output documents
   * @param configurationBucket The S3 bucket for configuration files
   * @param encryptionKey The KMS key for encryption
   * @param vpcConfiguration The VPC configuration
   * @returns The created Lambda data source
   */
  private addGetFileContentsDataSource(
    inputBucket: IBucket,
    outputBucket: IBucket,
    encryptionKey?: kms.IKey,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const getFileContentsResolverFunction =
      new functions.GetFileContentsResolverFunction(
        this,
        "GetFileContentsResolverFunction",
        {
          inputBucket: inputBucket,
          outputBucket: outputBucket,
          encryptionKey: encryptionKey,
          logGroup: new logs.LogGroup(this, "GetFileContentsResolverLogGroup", {
            encryptionKey: encryptionKey,
            retention: logs.RetentionDays.ONE_WEEK,
          }),
          ...vpcConfiguration,
        },
      );

    return this.addLambdaDataSource(
      "GetFileContentsDataSource",
      getFileContentsResolverFunction,
      {
        name: "GetFileContentsResolver",
        description: "Get the contents of a file from S3",
      },
    );
  }

  /**
   * Create Get File Contents Query Resolver using the provided data source.
   *
   * This method creates a resolver that handles file content retrieval queries
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for file content retrieval
   * @returns The created resolver
   */
  private createGetFileContentsQueryResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("GetFileContentsResolver", {
      dataSource: dataSource,
      typeName: "Query",
      fieldName: "getFileContents",
    });
  }

  /**
   * Add Get Step Function Execution Data Source to the GraphQL API.
   *
   * This method creates a Lambda data source for retrieving Step Functions execution details.
   * The data source can be used to create resolvers that allow clients to fetch
   * detailed execution information including step history, status, and error details.
   *
   * @param stateMachine The Step Functions state machine to query
   * @param encryptionKey The KMS key for encryption
   * @param logRetention The log retention period
   * @param vpcConfiguration The VPC configuration
   * @returns The created Lambda data source
   */
  private addGetStepFunctionExecutionDataSource(
    stateMachine: stepfunctions.IStateMachine,
    encryptionKey?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const getStepFunctionExecutionResolverFunction =
      new functions.GetStepFunctionExecutionResolverFunction(
        this,
        "GetStepFunctionExecutionResolverFunction",
        {
          stateMachine: stateMachine,
          encryptionKey: encryptionKey,
          logGroup: new logs.LogGroup(
            this,
            "GetStepFunctionExecutionResolverLogGroup",
            {
              encryptionKey: encryptionKey,
              retention: logRetention || logs.RetentionDays.ONE_WEEK,
            },
          ),
          ...vpcConfiguration,
        },
      );

    return this.addLambdaDataSource(
      "GetStepFunctionExecutionDataSource",
      getStepFunctionExecutionResolverFunction,
      {
        name: "GetStepFunctionExecutionResolver",
        description: "Get Step Functions execution details with step history",
      },
    );
  }

  /**
   * Create Get Step Function Execution Query Resolver using the provided data source.
   *
   * This method creates a resolver that handles Step Functions execution detail queries
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for Step Functions execution retrieval
   * @returns The created resolver
   */
  private createGetStepFunctionExecutionQueryResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("GetStepFunctionExecutionResolver", {
      dataSource: dataSource,
      typeName: "Query",
      fieldName: "getStepFunctionExecution",
    });
  }

  /**
   * Add Configuration Resolver for getting and updating configuration.
   *
   * This method adds resolvers to the GraphQL API for querying and updating
   * configuration settings stored in the provided configuration table.
   *
   * @param configurationTable The DynamoDB table for configuration storage
   * @param encryptionKey The KMS key for encryption
   * @param logRetention The log retention period
   * @param vpcConfiguration The VPC configuration
   */
  private addConfigurationResolver(
    configurationTable: IConfigurationTable,
    encryptionKey?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): void {
    const configurationResolverFunction =
      new functions.ConfigurationResolverFunction(
        this,
        "ConfigurationResolverFunction",
        {
          configurationTable: configurationTable,
          encryptionKey: encryptionKey,
          logGroup: new logs.LogGroup(
            this,
            "ConfigurationResolverFunctionLogGroup",
            {
              encryptionKey: encryptionKey,
              retention: logRetention || logs.RetentionDays.ONE_WEEK,
            },
          ),
          ...vpcConfiguration,
        },
      );

    // Create AppSync data source
    const configurationDataSource = this.addLambdaDataSource(
      "ConfigurationDataSource",
      configurationResolverFunction,
      {
        name: "ConfigurationDataSource",
        description: "Lambda function to manage configuration via GraphQL API",
      },
    );

    // Create resolvers
    this.createResolver("GetConfigurationResolver", {
      dataSource: configurationDataSource,
      typeName: "Query",
      fieldName: "getConfiguration",
    });

    this.createResolver("UpdateConfigurationResolver", {
      dataSource: configurationDataSource,
      typeName: "Mutation",
      fieldName: "updateConfiguration",
    });
  }

  /**
   * Add Delete Document Data Source to the GraphQL API.
   *
   * This method creates a Lambda data source for document deletion operations.
   * The data source can be used to create resolvers that allow clients to delete
   * documents from both the tracking table and the S3 buckets.
   *
   * @param trackingTable The DynamoDB table for tracking documents
   * @param inputBucket The S3 bucket for input documents
   * @param outputBucket The S3 bucket for output documents
   * @param encryptionKey The KMS key for encryption
   * @param logRetention The log retention period
   * @param vpcConfiguration The VPC configuration
   * @returns The created Lambda data source
   */
  private addDeleteDocumentDataSource(
    trackingTable: ITrackingTable,
    inputBucket: IBucket,
    outputBucket: IBucket,
    encryptionKey?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const deleteDocumentResolverFunction =
      new functions.DeleteDocumentResolverFunction(
        this,
        "DeleteDocumentResolverFunction",
        {
          trackingTable: trackingTable,
          inputBucket: inputBucket,
          outputBucket: outputBucket,
          encryptionKey: encryptionKey,
          logGroup: new logs.LogGroup(
            this,
            "DeleteDocumentResolverFunctionLogGroup",
            {
              encryptionKey: encryptionKey,
              retention: logRetention || logs.RetentionDays.ONE_WEEK,
            },
          ),
          ...vpcConfiguration,
        },
      );

    // Create and return AppSync data source
    return this.addLambdaDataSource(
      "DeleteDocumentDataSource",
      deleteDocumentResolverFunction,
      {
        name: "DeleteDocumentDataSource",
        description: "Lambda function for deleting documents",
      },
    );
  }

  /**
   * Create Delete Document Mutation Resolver using the provided data source.
   *
   * This method creates a resolver that handles document deletion mutations
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for document deletion
   * @returns The created resolver
   */
  private createDeleteDocumentMutationResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("DeleteDocumentResolver", {
      dataSource: dataSource,
      typeName: "Mutation",
      fieldName: "deleteDocument",
    });
  }

  /**
   * Add Copy To Baseline Data Source to the GraphQL API.
   *
   * This method creates a Lambda data source for copying processed documents
   * to the evaluation baseline bucket for use in accuracy evaluation.
   *
   * @param evaluationBaselineBucket The S3 bucket for evaluation baseline documents
   * @param outputBucket The S3 bucket for output documents
   * @param encryptionKey The KMS key for encryption
   * @param logRetention The log retention period
   * @param vpcConfiguration The VPC configuration
   * @returns The created Lambda data source
   */
  private addCopyToBaselineDataSource(
    evaluationBaselineBucket: IBucket,
    outputBucket: IBucket,
    encryptionKey?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const copyToBaselineResolverFunction =
      new functions.CopyToBaselineResolverFunction(
        this,
        "CopyToBaselineResolverFunction",
        {
          outputBucket: outputBucket,
          evaluationBaselineBucket: evaluationBaselineBucket,
          graphqlApiUrl: this.graphqlUrl,
          graphqlApiArn: this.arn,
          encryptionKey: encryptionKey,
          logGroup: new logs.LogGroup(
            this,
            "CopyToBaselineResolverFunctionLogGroup",
            {
              encryptionKey: encryptionKey,
              retention: logRetention || logs.RetentionDays.ONE_WEEK,
            },
          ),
          ...vpcConfiguration,
        },
      );

    return this.addLambdaDataSource(
      "CopyToBaselineDataSource",
      copyToBaselineResolverFunction,
      {
        name: "CopyToBaselineDataSource",
        description: "Lambda function for copying files to baseline bucket",
      },
    );
  }

  /**
   * Create Copy To Baseline Mutation Resolver using the provided data source.
   *
   * This method creates a resolver that handles document copying mutations
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for copying documents to baseline
   * @returns The created resolver
   */
  private createCopyToBaselineMutationResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("CopyToBaselineResolver", {
      dataSource: dataSource,
      typeName: "Mutation",
      fieldName: "copyToBaseline",
    });
  }

  /**
   * Add Reprocess Document Data Source to the GraphQL API.
   *
   * This method creates a Lambda data source for document reprocessing operations.
   * The data source can be used to create resolvers that allow clients to trigger
   * reprocessing of documents that have already been processed.
   *
   * @param inputBucket The S3 bucket for input documents
   * @param encryptionKey The KMS key for encryption
   * @param logRetention The log retention period
   * @param vpcConfiguration The VPC configuration
   * @returns The created Lambda data source
   */
  private addReprocessDocumentDataSource(
    inputBucket: IBucket,
    encryptionKey?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const reprocessDocumentResolverFunction =
      new functions.ReprocessDocumentResolverFunction(
        this,
        "ReprocessDocumentResolverFunction",
        {
          inputBucket: inputBucket,
          encryptionKey: encryptionKey,
          logGroup: new logs.LogGroup(
            this,
            "ReprocessDocumentResolverFunctionLogGroup",
            {
              encryptionKey: encryptionKey,
              retention: logRetention || logs.RetentionDays.ONE_WEEK,
            },
          ),
          ...vpcConfiguration,
        },
      );

    return this.addLambdaDataSource(
      "ReprocessDocumentDataSource",
      reprocessDocumentResolverFunction,
      {
        name: "ReprocessDocumentDataSource",
        description: "Lambda function for reprocessing documents",
      },
    );
  }

  /**
   * Create Reprocess Document Mutation Resolver using the provided data source.
   *
   * This method creates a resolver that handles document reprocessing mutations
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for document reprocessing
   * @returns The created resolver
   */
  private createReprocessDocumentMutationResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("ReprocessDocumentResolver", {
      dataSource: dataSource,
      typeName: "Mutation",
      fieldName: "reprocessDocument",
    });
  }

  /**
   * Add Process Changes Data Source to the GraphQL API.
   *
   * This method creates a Lambda data source for processing document section changes.
   * The data source can be used to create resolvers that allow clients to modify
   * document sections and trigger reprocessing.
   *
   * @param trackingTable The DynamoDB table for tracking document processing
   * @param documentQueue The SQS queue for document processing
   * @param workingBucket The S3 bucket for working files
   * @param inputBucket The S3 bucket for input documents
   * @param outputBucket The S3 bucket for output documents
   * @param dataRetentionInDays Data retention period in days
   * @param encryptionKey The KMS key for encryption
   * @param logRetention The log retention period
   * @param vpcConfiguration The VPC configuration
   * @returns The created Lambda data source
   */
  private addProcessChangesDataSource(
    trackingTable: ITrackingTable,
    documentQueue: IQueue,
    workingBucket: IBucket,
    inputBucket: IBucket,
    outputBucket: IBucket,
    dataRetentionInDays: number,
    encryptionKey?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const processChangesResolverFunction =
      new functions.ProcessChangesResolverFunction(
        this,
        "ProcessChangesResolverFunction",
        {
          trackingTable: trackingTable,
          documentQueue: documentQueue,
          workingBucket: workingBucket,
          inputBucket: inputBucket,
          outputBucket: outputBucket,
          appsyncApiUrl: this.graphqlUrl,
          graphqlApiArn: this.arn,
          dataRetentionInDays: dataRetentionInDays,
          encryptionKey: encryptionKey,
          logGroup: new logs.LogGroup(
            this,
            "ProcessChangesResolverFunctionLogGroup",
            {
              encryptionKey: encryptionKey,
              retention: logRetention || logs.RetentionDays.ONE_WEEK,
            },
          ),
          ...vpcConfiguration,
        },
      );

    return this.addLambdaDataSource(
      "ProcessChangesDataSource",
      processChangesResolverFunction,
      {
        name: "ProcessChangesDataSource",
        description: "Lambda function for processing section changes",
      },
    );
  }

  /**
   * Create Process Changes Mutation Resolver using the provided data source.
   *
   * This method creates a resolver that handles document section changes mutations
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for processing changes
   * @returns The created resolver
   */
  private createProcessChangesMutationResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("ProcessChangesResolver", {
      dataSource: dataSource,
      typeName: "Mutation",
      fieldName: "processChanges",
    });
  }

  /**
   * Add Upload Document Resolver to the GraphQL API.
   *
   * This method creates a resolver that generates presigned URLs
   * for uploading documents directly to S3 from the client.
   *
   * @param props The tracking API properties
   * @param environment The processing environment
   * @private
   */
  /**
   * Add Upload Document Data Source to the GraphQL API.
   *
   * This method creates a Lambda data source for generating presigned URLs
   * for uploading documents directly to S3 from the client.
   *
   * @param evaluationBaselineBucket The S3 bucket for evaluation baseline documents
   * @param inputBucket The S3 bucket for input documents
   * @param outputBucket The S3 bucket for output documents
   * @param encryptionKey The KMS key for encryption
   * @param logRetention The log retention period
   * @param vpcConfiguration The VPC configuration
   * @returns The created Lambda data source
   */
  private addUploadDocumentDataSource(
    inputBucket: IBucket,
    outputBucket: IBucket,
    evaluationBaselineBucket?: IBucket,
    encryptionKey?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const uploadResolverFunction = new functions.UploadResolverFunction(
      this,
      "UploadResolverFunction",
      {
        inputBucket: inputBucket,
        outputBucket: outputBucket,
        evaluationBaselineBucket: evaluationBaselineBucket,
        encryptionKey: encryptionKey,
        logGroup: new logs.LogGroup(this, "UploadResolverFunctionLogGroup", {
          encryptionKey: encryptionKey,
          retention: logRetention || logs.RetentionDays.ONE_WEEK,
        }),
        ...vpcConfiguration,
      },
    );

    return this.addLambdaDataSource(
      "UploadResolverDataSource",
      uploadResolverFunction,
      {
        name: "UploadResolverDataSource",
        description: "Lambda function for generating presigned URLs",
      },
    );
  }

  /**
   * Create Upload Document Mutation Resolver using the provided data source.
   *
   * This method creates a resolver that handles document upload mutations
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for document uploads
   * @returns The created resolver
   */
  private createUploadDocumentMutationResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("UploadDocumentResolver", {
      dataSource: dataSource,
      typeName: "Mutation",
      fieldName: "uploadDocument",
    });
  }

  /**
   * Add Query Knowledge Base Data Source to the GraphQL API.
   *
   * This method creates a Lambda data source for querying the document
   * knowledge base using natural language. It integrates with Amazon Bedrock
   * to provide intelligent document search capabilities.
   *
   * @param knowledgeBase The Bedrock knowledge base to query
   * @param logLevel The log level for the resolver
   * @param key The KMS key for encryption
   * @param logRetention The log retention period
   * @param vpcConfiguration The VPC configuration
   * @returns The created Lambda data source
   */
  private addQueryKnowledgeBaseDataSource(
    knowledgeBase: IKnowledgeBase,
    knowledgeBaseModel: IInvokable,
    knowledgeBaseGuardrail?: IGuardrail,
    logLevel?: LogLevel,
    key?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const queryKnowledgeBaseResolverFunction =
      new functions.QueryKnowledgeBaseResolverFunction(
        this,
        "QueryKnowledgeBaseResolverFunction",
        {
          knowledgeBase: knowledgeBase,
          knowledgeBaseModel: knowledgeBaseModel,
          guardrail: knowledgeBaseGuardrail,
          logLevel: logLevel ?? LogLevel.INFO,
          encryptionKey: key,
          logGroup: new logs.LogGroup(
            this,
            "QueryKnowledgeBaseResolverFunctionLogGroup",
            {
              encryptionKey: key,
              retention: logRetention || logs.RetentionDays.ONE_WEEK,
            },
          ),
          ...vpcConfiguration,
        },
      );

    return this.addLambdaDataSource(
      "QueryKnowledgeBaseDataSource",
      queryKnowledgeBaseResolverFunction,
      {
        name: "QueryKnowledgeBase",
        description: "Lambda function to query Bedrock Knowledge Base",
      },
    );
  }

  /**
   * Create Query Knowledge Base Query Resolver using the provided data source.
   *
   * This method creates a resolver that handles knowledge base queries
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for knowledge base queries
   * @returns The created resolver
   */
  private createQueryKnowledgeBaseQueryResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("QueryKnowledgeBaseResolver", {
      dataSource: dataSource,
      typeName: "Query",
      fieldName: "queryKnowledgeBase",
    });
  }

  /**
   * Add data sources and resolvers for Agent Analytics functionality.
   *
   * This method creates all the necessary AppSync data sources and resolvers
   * for agent analytics including agent request handling, job processing,
   * and listing available agents.
   */
  private addAgentAnalyticsDataSources(): void {
    if (!this._agentAnalytics) {
      throw new Error(
        "Agent Analytics must be initialized before adding data sources",
      );
    }

    // Add Agent Request Handler data source
    const agentRequestHandlerDataSource = this.addLambdaDataSource(
      "AgentRequestHandlerDataSource",
      this._agentAnalytics.agentRequestHandler,
      {
        name: "AgentRequestHandler",
        description: "Lambda function to handle agent query requests",
      },
    );

    // Add List Available Agents data source
    const listAvailableAgentsDataSource = this.addLambdaDataSource(
      "ListAvailableAgentsDataSource",
      this._agentAnalytics.listAvailableAgents,
      {
        name: "ListAvailableAgents",
        description: "Lambda function to list available analytics agents",
      },
    );

    // Add Agent Table data source for job status queries
    const agentTableDataSource = this.addDynamoDbDataSource(
      "AgentTableDataSource",
      this._agentAnalytics.agentTable,
    );

    // Create resolvers
    this.createResolver("SubmitAgentQueryResolver", {
      dataSource: agentRequestHandlerDataSource,
      typeName: "Query",
      fieldName: "submitAgentQuery",
    });

    this.createResolver("ListAvailableAgentsResolver", {
      dataSource: listAvailableAgentsDataSource,
      typeName: "Query",
      fieldName: "listAvailableAgents",
    });

    // Create getAgentJobStatus resolver using DynamoDB data source
    agentTableDataSource.createResolver("GetAgentJobStatusResolver", {
      typeName: "Query",
      fieldName: "getAgentJobStatus",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
        #set($userId = $context.identity.username)
        #if(!$userId)
          #set($userId = $context.identity.sub)
        #end
        #if(!$userId)
          #set($userId = "anonymous")
        #end
        {
          "version": "2018-05-29",
          "operation": "GetItem",
          "key": {
            "PK": $util.dynamodb.toDynamoDBJson("agent#\${userId}"),
            "SK": $util.dynamodb.toDynamoDBJson($ctx.args.jobId)
          }
        }
      `),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`
        #if(!$ctx.result)
          null
        #else
          {
            "jobId": $util.toJson($ctx.result.SK),
            "status": $util.toJson($ctx.result.status),
            "query": $util.toJson($ctx.result.query),
            "agentIds": $util.toJson($ctx.result.agentIds),
            "createdAt": $util.toJson($ctx.result.createdAt),
            "completedAt": $util.toJson($ctx.result.completedAt),
            "result": $util.toJson($ctx.result.result),
            "error": $util.toJson($ctx.result.error),
            "agent_messages": $util.toJson($ctx.result.agent_messages)
          }
        #end
      `),
    });

    // Create updateAgentJobStatus resolver using DynamoDB data source
    agentTableDataSource.createResolver("UpdateAgentJobStatusResolver", {
      typeName: "Mutation",
      fieldName: "updateAgentJobStatus",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
        #set($userId = $ctx.args.userId)
        #set($expNames = {})
        #set($expValues = {})
        
        ## Set status (required)
        $util.qr($expNames.put("#status", "status"))
        $util.qr($expValues.put(":status", $util.dynamodb.toDynamoDB($ctx.args.status)))
        
        ## Set result if provided
        #if($ctx.args.result)
          $util.qr($expNames.put("#result", "result"))
          $util.qr($expValues.put(":result", $util.dynamodb.toDynamoDB($ctx.args.result)))
        #end
        
        ## Set completedAt timestamp
        $util.qr($expNames.put("#completedAt", "completedAt"))
        $util.qr($expValues.put(":completedAt", $util.dynamodb.toDynamoDB($util.time.nowISO8601())))
        
        {
          "version": "2018-05-29",
          "operation": "UpdateItem",
          "key": {
            "PK": $util.dynamodb.toDynamoDBJson("agent#\${userId}"),
            "SK": $util.dynamodb.toDynamoDBJson($ctx.args.jobId)
          },
          "update": {
            "expression": "SET #status = :status, #completedAt = :completedAt#if($ctx.args.result), #result = :result#end",
            "expressionNames": $util.toJson($expNames),
            "expressionValues": $util.toJson($expValues)
          }
        }
      `),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`
        #if($ctx.error)
          $util.error($ctx.error.message, $ctx.error.type)
        #end
        
        ## Return false if no item was updated (item not found)
        #if(!$ctx.result)
          false
        #else
          true
        #end
      `),
    });

    // Create listAgentJobs resolver using DynamoDB data source
    agentTableDataSource.createResolver("ListAgentJobsResolver", {
      typeName: "Query",
      fieldName: "listAgentJobs",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
        #set($userId = $context.identity.username)
        #if(!$userId)
          #set($userId = $context.identity.sub)
        #end
        #if(!$userId)
          #set($userId = "anonymous")
        #end
        {
          "version": "2018-05-29",
          "operation": "Query",
          "query": {
            "expression": "PK = :pk",
            "expressionValues": {
              ":pk": $util.dynamodb.toDynamoDBJson("agent#\${userId}")
            }
          },
          #if($ctx.args.limit)
            "limit": $ctx.args.limit,
          #end
          #if($ctx.args.nextToken)
            "nextToken": "$ctx.args.nextToken",
          #end
          "scanIndexForward": false
        }
      `),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`
        {
          "items": [
            #foreach($item in $ctx.result.items)
              {
                "jobId": $util.toJson($item.SK),
                "status": $util.toJson($item.status),
                "query": $util.toJson($item.query),
                "agentIds": $util.toJson($item.agentIds),
                "createdAt": $util.toJson($item.createdAt),
                "completedAt": $util.toJson($item.completedAt),
                "result": $util.toJson($item.result),
                "error": $util.toJson($item.error)
              }#if($foreach.hasNext),#end
            #end
          ],
          "nextToken": $util.toJson($ctx.result.nextToken)
        }
      `),
    });

    // Create deleteAgentJob resolver using DynamoDB data source
    agentTableDataSource.createResolver("DeleteAgentJobResolver", {
      typeName: "Mutation",
      fieldName: "deleteAgentJob",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
        #set($userId = $context.identity.username)
        #if(!$userId)
          #set($userId = $context.identity.sub)
        #end
        #if(!$userId)
          #set($userId = "anonymous")
        #end
        {
          "version": "2018-05-29",
          "operation": "DeleteItem",
          "key": {
            "PK": $util.dynamodb.toDynamoDBJson("agent#\${userId}"),
            "SK": $util.dynamodb.toDynamoDBJson($ctx.args.jobId)
          }
        }
      `),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`
        #if($ctx.error)
          $util.error($ctx.error.message, $ctx.error.type)
        #else
          true
        #end
      `),
    });
  }

  /**
   * Add Chat with Document data source.
   *
   * This method creates a Lambda data source for chat with document functionality
   * and configures the necessary permissions and environment variables.
   *
   * @param knowledgeBase The Bedrock knowledge base for document context
   * @param chatModel The invokable model for chat functionality
   * @param guardrail Optional Bedrock guardrail for content filtering
   * @param logLevel The log level for the function
   * @param key Optional KMS key for encryption
   * @param logRetention Log retention period
   * @param vpcConfiguration Optional VPC configuration
   * @returns The created Lambda data source
   */
  private addChatWithDocumentDataSource(
    knowledgeBase: IKnowledgeBase,
    chatModel: IInvokable,
    guardrail?: IGuardrail,
    logLevel?: LogLevel,
    key?: kms.IKey,
    logRetention?: logs.RetentionDays,
    vpcConfiguration?: VpcConfiguration,
  ): appsync.LambdaDataSource {
    const chatWithDocumentResolverFunction =
      new functions.ChatWithDocumentResolverFunction(
        this,
        "ChatWithDocumentResolverFunction",
        {
          knowledgeBase: knowledgeBase,
          chatModel: chatModel,
          guardrail: guardrail,
          logLevel: logLevel ?? LogLevel.INFO,
          encryptionKey: key,
          logGroup: new logs.LogGroup(
            this,
            "ChatWithDocumentResolverFunctionLogGroup",
            {
              encryptionKey: key,
              retention: logRetention || logs.RetentionDays.ONE_WEEK,
            },
          ),
          ...vpcConfiguration,
        },
      );

    return this.addLambdaDataSource(
      "ChatWithDocumentDataSource",
      chatWithDocumentResolverFunction,
      {
        name: "ChatWithDocument",
        description: "Lambda function for chat with document functionality",
      },
    );
  }

  /**
   * Create Chat with Document Query Resolver using the provided data source.
   *
   * This method creates a resolver that handles chat with document queries
   * using the specified Lambda data source.
   *
   * @param dataSource The Lambda data source for chat with document
   * @returns The created resolver
   */
  private createChatWithDocumentQueryResolver(
    dataSource: appsync.LambdaDataSource,
  ): appsync.Resolver {
    return this.createResolver("ChatWithDocumentResolver", {
      dataSource: dataSource,
      typeName: "Query",
      fieldName: "chatWithDocument",
    });
  }

  /**
   * Add Document Discovery capabilities to the GraphQL API.
   *
   * This method adds document discovery functionality including automated
   * document analysis and configuration generation capabilities.
   *
   * @param documentDiscovery The document discovery construct with table, queue, and functions
   */
  public addDocumentDiscovery(documentDiscovery: IDocumentDiscovery): void {
    // Initialize functions with API URL and environment settings
    const { uploadResolverFunction } = documentDiscovery.initializeFunctions(
      this,
      this._configurationTable,
      this._encryptionKey,
      this._logLevel,
      this._logRetention,
      this._vpcConfiguration,
    );

    // Add upload discovery document resolver
    const discoveryUploadDataSource = this.addLambdaDataSource(
      "DiscoveryUploadDataSource",
      uploadResolverFunction,
      {
        name: "DiscoveryUploadResolver",
        description: "Lambda function for discovery document uploads",
      },
    );

    this.createResolver("UploadDiscoveryDocumentResolver", {
      dataSource: discoveryUploadDataSource,
      typeName: "Mutation",
      fieldName: "uploadDiscoveryDocument",
    });

    // Add discovery table data source for queries
    const discoveryTableDataSource = this.addDynamoDbDataSource(
      "DiscoveryTableDataSource",
      documentDiscovery.discoveryTable,
    );

    // Create list discovery jobs resolver
    discoveryTableDataSource.createResolver("ListDiscoveryJobsResolver", {
      typeName: "Query",
      fieldName: "listDiscoveryJobs",
      requestMappingTemplate: appsync.MappingTemplate.fromString(`
        {
          "version": "2017-02-28",
          "operation": "Scan",
          "limit": $util.defaultIfNull($ctx.args.limit, 20),
          "nextToken": $util.toJson($util.defaultIfNullOrBlank($ctx.args.nextToken, null))
        }
      `),
      responseMappingTemplate: appsync.MappingTemplate.fromString(`
        {
          "DiscoveryJobs": $util.toJson($ctx.result.items),
          "nextToken": $util.toJson($util.defaultIfNullOrBlank($ctx.result.nextToken, null))
        }
      `),
    });

    // Create update discovery job status resolver (for internal use)
    discoveryTableDataSource.createResolver(
      "UpdateDiscoveryJobStatusResolver",
      {
        typeName: "Mutation",
        fieldName: "updateDiscoveryJobStatus",
        requestMappingTemplate: appsync.MappingTemplate.fromString(`
          ## Validate status is one of the allowed values
          #set($validStatuses = ["PENDING", "IN_PROGRESS", "COMPLETED", "FAILED"])
          #if(!$validStatuses.contains($ctx.args.status))
            $util.error("Invalid status value. Status must be one of: PENDING, IN_PROGRESS, COMPLETED, FAILED", "ValidationException")
          #end
          
          #set($expNames = {})
          #set($expValues = {})
          
          ## Set status (required)
          $util.qr($expNames.put("#status", "status"))
          $util.qr($expValues.put(":status", $util.dynamodb.toDynamoDB($ctx.args.status)))
          #set($updateExpression = "SET #status = :status")
          
          ## Set errorMessage (optional)
          #if($ctx.args.errorMessage)
            $util.qr($expNames.put("#errorMessage", "errorMessage"))
            $util.qr($expValues.put(":errorMessage", $util.dynamodb.toDynamoDB($ctx.args.errorMessage)))
            #set($updateExpression = "\${updateExpression}, #errorMessage = :errorMessage")
          #end
          
          ## Set updatedAt to current timestamp
          $util.qr($expNames.put("#updatedAt", "updatedAt"))
          $util.qr($expValues.put(":updatedAt", $util.dynamodb.toDynamoDB($util.time.nowISO8601())))
          #set($updateExpression = "\${updateExpression}, #updatedAt = :updatedAt")
          
          ## Set completedAt when status is COMPLETED or FAILED
          #if($ctx.args.status == "COMPLETED" || $ctx.args.status == "FAILED")
            $util.qr($expNames.put("#completedAt", "completedAt"))
            $util.qr($expValues.put(":completedAt", $util.dynamodb.toDynamoDB($util.time.nowISO8601())))
            #set($updateExpression = "\${updateExpression}, #completedAt = :completedAt")
          #end
          
          {
            "version": "2018-05-29",
            "operation": "UpdateItem",
            "key": {
              "jobId": $util.dynamodb.toDynamoDBJson($ctx.args.jobId)
            },
            "update": {
              "expression": "$updateExpression",
              "expressionNames": $util.toJson($expNames),
              "expressionValues": $util.toJson($expValues)
            }
          }
        `),
        responseMappingTemplate: appsync.MappingTemplate.fromString(`
          $util.toJson($ctx.result)
        `),
      },
    );
  }
}
